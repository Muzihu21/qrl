# -*- coding: utf-8 -*-
"""train_qlearning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W863RgoMLCIOFmb-dbt93OUDrlT09me4
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
from env_qlearning import PenjualanEnv

# ==================================================
# ‚ö°Ô∏è 1. Environment dengan Reward Sederhana
# ==================================================
class EnhancedPenjualanEnv(PenjualanEnv):
    def __init__(self, data_path, max_steps=10):
        self.data_path = data_path
        super().__init__(data_path, max_steps)
        self.original_profit_mean = self._load_baseline()
        self.actions = getattr(self, 'actions', ['low', 'medium', 'high'])
        self.states = getattr(self, 'states', ['s1', 's2', 's3'])

    def _load_baseline(self):
        try:
            df = pd.read_csv(self.data_path)
            return df['profit'].mean() if 'profit' in df else 50000
        except:
            return 50000

    def calculate_reward(self, profit):
        """Reward = Normalisasi Profit + Bonus"""
        return (profit - self.original_profit_mean) * 10 + 100

    def step(self, action):
        next_state, profit, done = super().step(action)
        reward = self.calculate_reward(profit)
        return next_state, reward, done


# ==================================================
# ‚ö°Ô∏è 2. Q-learning Agent
# ==================================================
class QLearningAgent:
    def __init__(self, n_states, n_actions, lr=0.1, gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):
        self.q_table = np.zeros((n_states, n_actions))
        self.lr = lr
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        self.n_actions = n_actions

    def choose_action(self, state, training=True):
        if training and np.random.rand() < self.epsilon:
            return np.random.randint(self.n_actions)
        return np.argmax(self.q_table[state])

    def update(self, state, action, reward, next_state, done):
        target = reward
        if not done:
            target += self.gamma * np.max(self.q_table[next_state])
        self.q_table[state, action] += self.lr * (target - self.q_table[state, action])
        if done and self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay


# ==================================================
# ‚ö°Ô∏è 3. Training
# ==================================================
def train(output_dir="output", episodes=1000, max_steps=10):
    """Latih Q-learning Agent dan Simpan Model + Metadata"""
    # Init
    env = EnhancedPenjualanEnv("env_SAR.csv", max_steps=max_steps)
    state_idx = {s: i for i, s in enumerate(env.states)}
    agent = QLearningAgent(len(env.states), len(env.actions), lr=0.15, gamma=0.98, epsilon=1.0, epsilon_min=0.05, epsilon_decay=0.999)

    profits = []
    for ep in range(episodes):
        state = env.reset()
        s = state_idx[state]
        total_profit = 0
        done = False

        while not done:
            action_idx = agent.choose_action(s)
            action = env.actions[action_idx]

            next_state, reward, done = env.step(action)
            ns = state_idx[next_state]

            agent.update(s, action_idx, reward, ns, done)

            s = ns
            total_profit += reward

        profits.append(total_profit)

        if (ep + 1) % 200 == 0:
            avg_profit = np.mean(profits[-100:])
            print(f"Episode {ep + 1}, Avg Profit (last 100): {avg_profit:.2f}, Epsilon: {agent.epsilon:.3f}")

    # ==================================================
    # ‚ö°Ô∏è 4. Simpan Model, Profits, State Index, States, dan Baseline Profit
    # ==================================================
    os.makedirs(output_dir, exist_ok=True)

    np.save(f"{output_dir}/enhanced_q_table.npy", agent.q_table)
    np.save(f"{output_dir}/enhanced_profits.npy", np.array(profits))
    np.save(f"{output_dir}/state_idx.npy", state_idx)

    with open(f"{output_dir}/states.txt", "w") as f:
        for state in env.states:
            f.write(f"{state}\n")

    with open(f"{output_dir}/baseline_profit.txt", "w") as f:
        f.write(str(env.original_profit_mean))

    # ==================================================
    # ‚ö°Ô∏è 5. Plot Profits
    # ==================================================
    plt.figure(figsize=(12, 5))
    plt.plot(profits, label='Profit per Episode', color='deepskyblue', alpha=0.7)
    plt.xlabel('Episode'); plt.ylabel('Profit')
    plt.title('Training Profit Progression')
    plt.grid(True)
    plt.legend()
    plt.savefig(f"{output_dir}/enhanced_qlearning_results.png", dpi=300)
    plt.show()

    # ==================================================
    # ‚ö°Ô∏è 6. Done
    # ==================================================
    print("\n‚úÖ Training Complete!\nFiles saved to:", output_dir)


if __name__ == "__main__":
    print("üîß Running Simplified & Exportable Q-learning...")
    train(output_dir="output", episodes=1000, max_steps=10)

# ==================================================
# üìä PLOTTING
# ==================================================
import numpy as np
import matplotlib.pyplot as plt

episodes = len(profits)
window_size = 100
avg_rewards = [np.mean(profits[i - window_size:i]) for i in range(window_size, episodes + 1)]

plt.figure(figsize=(12,5))
plt.plot(range(window_size, episodes + 1), avg_rewards, color="tomato", label=f"Rata-rata Profit per {window_size} Episode")
plt.title(f"Rata-rata Profit per {window_size} Episode")
plt.xlabel("Episode"); plt.ylabel("Average Profit"); plt.grid(True)
plt.legend()
plt.show()

